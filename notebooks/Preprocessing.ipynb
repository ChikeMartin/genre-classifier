{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../raw_data/genre/genres_original/\"\n",
    "JSON_PATH = \"../raw_data/genre/data.json \"\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30 # seconds per track\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '../raw_data/genre/genres_original/hiphop/hiphop.00007.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(filename= _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function created to walk through path, extract labels, perform STFT tranforms on audio files and store the MFCCs from those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path, json_path, n_mfcc= 13, n_fft= 2048, hop_length= 512, num_segments= 5):\n",
    "    SAMPLE_RATE = 22050\n",
    "    DURATION = 30 # seconds per track\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
    "    \n",
    "    # dictionary to store data\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_nmfcc_vectors_per_segment = math.ceil(num_samples_per_segment / hop_length)\n",
    "    \n",
    "    # loop through all genres\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        # ensure we are not at root level\n",
    "        if dirpath is not dataset_path:\n",
    "            # save the semantic label\n",
    "            semantic_label = dirpath.split(\"/\")[-1] # genre/blues => ['genre', 'blues']\n",
    "            data['mapping'].append(semantic_label)\n",
    "            print(f\"\\nProcessing {semantic_label}\")\n",
    "            \n",
    "            # process files for a specific genre\n",
    "            for f in filenames:\n",
    "                # load audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "                \n",
    "                # process by segment, extracting mfcc and storing data\n",
    "                for s in range(num_segments):\n",
    "                    start_sample = num_samples_per_segment * s \n",
    "                    finish_sample = start_sample + num_samples_per_segment\n",
    "                    \n",
    "                    mfcc = librosa.feature.mfcc(signal[start_sample: finish_sample],\n",
    "                                                sr=sr,\n",
    "                                                n_fft= n_fft,\n",
    "                                                n_mfcc= n_mfcc,\n",
    "                                                hop_length = hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "                    \n",
    "                    # store mfcc for segment if it has the expected length\n",
    "                    if len(mfcc) == expected_nmfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i-1)\n",
    "                        print(f\"{file_path}, segment: {s+1}\")\n",
    "                        \n",
    "        with open(json_path, 'w') as fp:\n",
    "            json.dump(data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result stored as `.json` file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "        \n",
    "    # convert lists to np.array e.g 'mapping', 'labels' in json file\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = load_data(JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8996, 130, 13)\n",
      "(8996,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label for blues: 0\n",
      "Target label for classical: 1\n",
      "Target label for country: 2\n",
      "Target label for disco: 3\n",
      "Target label for hiphop: 4\n",
      "Target label for metal: 5\n",
      "Target label for pop: 6\n",
      "Target label for reggae: 7\n",
      "Target label for rock: 8\n"
     ]
    }
   ],
   "source": [
    "for semantic_label, target_label in zip(semantic, np.unique(targets)):\n",
    "    print(f\"Target label for {semantic_label}: {target_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple `train_test_split` can be performed on the data for the baseline multilayer peceptron model. \n",
    "\n",
    "The following function would be used to split data into `train | test | validation` for the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(test_size=0.25, validation_size= 0.2):\n",
    "    \n",
    "    # load_data\n",
    "    X, y, smeantic = load_data(JSON_PATH)\n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    \n",
    "    # train / val split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "    \n",
    "    # 3D array for each sample eg --> (130, 13, 1)\n",
    "    X_train = X_train[..., np.newaxis] # 4d array --> (num_samples, 130, 13, 1)\n",
    "    X_val =X_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
